最后一讲：

 

规则先做好，先generic 然后再上模型，解决特殊的问题

![image-20220430130025088](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130025088.png)

![image-20220430130037786](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130037786.png)

![image-20220430130045295](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130045295.png)



![image-20220430130053962](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130053962.png)



我们见过这个形状了，能不能根据形状，figure out出一个最优的形状

 

在给了reward function情况下，看given当前状态到action的一个mapping

 

RL 建立这样一个mapping

 

语文哪里号，哪里不好，不好的地方改进，rl的思想， 

 

随机的去看，然后修正，optim func 到所有state

 

但环境剧烈变化！

---怎么解决---

![image-20220430130112170](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130112170.png)

不是完全感知！

 

Pomdp

 

Action 到mapping --imitation learning！

 

对大的数据--后话！

eetv要多少个数据

![image-20220430130138524](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130138524.png)

![image-20220430130153052](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130153052.png)

![image-20220430130203118](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130203118.png)

![image-20220430130211361](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130211361.png)

![image-20220430130221000](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130221000.png)

![image-20220430130229258](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130229258.png)



![image-20220430130237907](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130237907.png)

Data driven 只是加速了过程

![image-20220430130254742](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130254742.png)

Long term goal short term goal

 

 

对一个模型，总是想问，他的reward function是什么样的

 

影响非常大！

 

决策，reward多大，打不打ta

 

但强化学习，不是reward function 而是给目标函数

取learning mapping

 

不会噶苏你怎么调的

 

Reward function都是收条

Irl 和 an drew ng 文章 band shape learning while inverse R learing

![image-20220430130321771](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130321771.png)

 

迭代套迭代，所以学的满，你跟好同学比较也是

![image-20220430130345216](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130345216.png)

Diff 用pociy 修正， 

Imitate learing 是不generic

而rl是 generic

![image-20220430130404195](/Users/lixiang/Documents/typora/learn/0415learnthing/planning7.assets/image-20220430130404195.png) 

Open ai完





























